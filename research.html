
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <link rel="stylesheet" type="text/css" href="./style.css">
    <title>Jiangyu Hu (WUST)</title>
    <base href=".">
</head>

<body data-new-gr-c-s-check-loaded="14.1022.0" data-gr-ext-installed="">
<h1 style="padding-left: 0.5em">Jiangyu Hu (WUST)</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="https://syc-hjy.github.io/index.html">Home</a></div>
	<div class="menu-item"><a href="https://syc-hjy.github.io/research.html" class="current">Publications</a></div>
	<div class="menu-item"><a href="https://syc-hjy.github.io/services.html">Professional Services</a></div>
    <div class="menu-item"><a href="https://syc-hjy.github.io/codedata.html">Code &amp; Datasets</a></div>
	<div class="menu-item"><a href="https://syc-hjy.github.io/news.html">News</a></div>
</td>
<td id="layout-content">

<div>
        <h1 style="margin-top: 0em">Research</h1><br>

<!--<ul>-->
<p>My general research interests lie in multimodal fusion and embodied intelligence and deep resnet reinforcement learning, with a particular emphasis on the following topics:</p>
<!--</ul>-->
<ul>
<li><p><b>Theory:</b><br>
Analysis and understandings of the generalization ability of multimodal loss functions.</p></li>
<li><p><b>Algorithm:</b><br>
Development of effective deep learning algorithms for robotic multimodal fusion and resnet reinforcement learning.</p></li>
<li><p><b>Application:</b><br>
Application of learning algorithms in embodied intelligence, assembly, and human-robot contact rich manipulation.</p></li>
</ul>
<p>Feel free to drop me an email if you are interested in collaborating with me on the above research topics.</p>

</div>

	<div>
    <h2><hr>Publications</h2>
    <p>(* Corresponding author; † Equal contribution)</p>
    <ul>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            VITaL Pretraining: Visuo-Tactile Pretraining for Tactile and Non-Tactile Manipulation Policies.<br>
            <i>The IEEE International Conference on Automation Science and Engineering (CASE).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals.<br>
            <i>The International Conference on Biomimetic Intelligence and Robotics (ICBIR).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    Shaobo Yang, Hongtong Li, <b>Jiangyu Hu</b>, Shixin Zhang, Guocai Yao, Ziqiang Ni, Bin Fang.<br>
            BiTLA: A Bimanual Tactile-Language-Action Model for Contact-Rich Robotic Manipulation.<br>
            <i>1st International Workshop on Multi-Sensorial Media and Applications (ACMMMW).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    Ruoxuan Feng, <b>Jiangyu Hu</b>, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Bin Fang, Di Hu.<br>
            AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors.<br>
            <i>The Thirteenth International Conference on Learning Representations (ICLR).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            XR Interface to Enhance the Learning Feeling in Arc Welding Training Tasks.<br>
            <i>43rd Chinese Control Conference (CCC).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            QwenGrasp: A Usage of Large Vision Language Model for Target-oriented Grasping.<br>
            <i>2024 Chinese Control Conference (CAC).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            Neural contact fields: Tracking extrinsic contact with tactile sensing.<br>
            <i>2024 IEEE International Conference on Robotics and Biomimetics  (ROBIO).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            Tac-VGNN: A Voronoi Graph Neural Network for Pose-Based Tactile Servoing.<br>
            <i>2024 IEEE International Conference on Robotics and Biomimetics  (ROBIO).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutters.<br>
            <i>The 5th China Intelligent Robotics Academic Conference (CIRAC).<br>
        </p></li>
	<li><p style="margin:15px 0">
	    <b>Jiangyu Hu</b>, Huasong Min.<br>
            Octopi: Object Property Reasoning with Large Tactile-Language Models.<br>
            <i>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).<br>
        </p></li>
<!-- 	<li><p style="margin:15px 0">
	    <b>Xin Yuan</b>, Xin Xu, Xiao Wang, Liang Liao, Kai Zhang, Zheng Wang, Chia-Wen Lin.<br>
            OSAP-Loss: Efficient Optimization of Average Precision via Involving Samples after Positive Ones Towards Remote Sensing Image Retrieval.<br>
            <i>CAAI Transactions on Intelligence Technology</i>, pp. 1-22, 2023.<br>
        </p></li>
        <li><p style="margin:15px 0">
            Xin Xu, <b>Xin Yuan</b>, Zheng Wang, Kai Zhang, Ruimin Hu.<br>
            Rank-in-Rank Loss for Person Re-identification.<br>
            <i>ACM Transactions on Multimedia Computing, Communications, and Applications</i>, vol. 18, no. 2s, art. no. 130, pp. 1-21, 2022.<br>
        </p></li>
	<li><p style="margin:15px 0">
            Xiang Shuai, Xiao Wang, Wei Wang, <b>Xin Yuan</b>, Xin Xu.<br>
            SAM: Self Attention Mechanism for Scene Text Recognition based on Swin Transformer.<br>
	    <i>Proceedings of the 28th International Conference on Multimedia Modeling (<b>MMM'22</b>)</i>, pp. 443–454, 2022.<br>
         </p></li> -->
    </ul>

    </div>

</td>
</tr>
</tbody></table>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
